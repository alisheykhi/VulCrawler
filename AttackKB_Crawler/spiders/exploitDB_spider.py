# -*- coding: utf-8 -*-
# Author : ali.sheykhi@me.com


from datetime import datetime
import scrapy,re,os
import base64
import hashlib

from attackKB_crawler.items.exploitdb_items import ExploitDbItem

class EDBSpider(scrapy.Spider):
    handle_httpstatus_list = [500,403]
    name = 'Exploit_DB'
    allowed_domains = ['exploit-db.com']
    last_date = datetime.strptime('2000-01-01', '%Y-%m-%d')
    start_urls = []
    #update_list_url = []
    custom_settings = {
        'ITEM_PIPELINES': {
            #'attackKB_crawler.pipelines.securityfocus_pipelines.SecurityFocusJsonWriterPipeline': 900,
            'attackKB_crawler.pipelines.post_pipeline.PostPipeline': 901,
        },
    }
    pages = ["remote","webapps","local","dos","shellcode","papers"]
    #pages = ["papers"]
    table_xpath = {"Title" : "//div[@class='l-titlebar-content']/h1/text()",
                   "EDB_ID": "//table[@class='exploit_list']//td[contains(., 'EDB-ID')]/text()",
                   "AuthorName": "//table[@class='exploit_list']//td[contains(., 'Author')]/a/text()",
                   "AuthorLink": "//table[@class='exploit_list']//td[contains(., 'Author')]/a/@href",
                   "Published": "//table[@class='exploit_list']//td[contains(., 'Published')]/text()",
                   "CVE": "//table[@class='exploit_list']//td[contains(., 'CVE')]/a/text()",
                   "Type": "//table[@class='exploit_list']//td[contains(., 'Type')]/a/text()",
                   "Platform": "//table[@class='exploit_list']//td[contains(., 'Platform')]/a/text()",
                   "Aliases": "//table[@class='exploit_list']//td[contains(., 'Aliases')]/text()",
                   "AdvisorySource": "//table[@class='exploit_list']//td[contains(., 'Advisory/Source')]/a/@href",
                   "Tags": "//table[@class='exploit_list']//td[contains(., 'Tags')]/text()",
                   "E_DBVerified": "//table[@class='exploit_list']//td[contains(., 'E-DB Verified')]/img/@title",
                   "ExploitLink": "//table[@class='exploit_list']//td[contains(., 'Exploit')]/a[2]",
                   "VulnerableApp": "//table[@class='exploit_list']//td[contains(., 'Vulnerable App')]/a/@href",
                   "ScreenShot": "//div[@class='exploit-meta clearfix']/div[@class='screenshot']/a/@href",
                   "Exploit": "//div[@id='container']//text()",}

    def __init__(self,callback, ldate='2017-01-01', *args, **kwargs):
        super(EDBSpider, self).__init__(*args, **kwargs)
        self.last_date = datetime.strptime(ldate, '%Y-%m-%d')
        self.callback= callback

    def start_requests(self):
        for page in self.pages:
            yield scrapy.Request("https://www.exploit-db.com/%s/?order_by=date_published&order=desc&pg=1" %page , self.get_update_list)

    def get_update_list(self,response):
        date_xpath = "//table[@class='exploit_list bootstrap-wrapper']/tbody/tr/td[@class='date']/text()"
        url_xpath = "//table[@class='exploit_list bootstrap-wrapper']/tbody/tr/td[@class='description']/a/@href"
        counter = 1
        page_number = int(str(response.url).split("=")[-1])
        date_list = response.xpath(date_xpath).extract()
        url_list = response.xpath(url_xpath).extract()
        for index in range(0,len(date_list)):
            if self.last_date <= datetime.strptime(date_list[index], '%Y-%m-%d'):
                #self.update_list_url.append(url_list[index])
                yield scrapy.Request(url_list[index],self.parse_page)
                if counter == 50:
                    next_page = re.sub('\d*$',str(page_number+1),response.url)
                    yield scrapy.Request(next_page , self.get_update_list)
                else:
                    counter += 1

    def extract_item(self,response, xpath_val):
        try:
            return str(re.sub("\s\s+", "", "".join(response.xpath(xpath_val).extract())))
        except:
            return ""

    def parse_page(self, response):
        info = ExploitDbItem()

        if response.status in self.handle_httpstatus_list: #check  response.url
            self.logger.error("404 error : ",response.url)
            print " \n ********** page not found **********"
        if (str(response.url).split(".")[-1] == "pdf"):

            info ["EDB_ID"] = re.sub(".pdf","",(response.url).split("/")[-1])
            pdf = (response.body)
            pdf_base64 = base64.b64encode(response.body)
            info ["Article"] = pdf_base64
            info ["ArticleHash"] = hashlib.md5(pdf_base64).hexdigest()
            # directory = os.getcwd() + "/ExploitDB/PDF/"
            # if not os.path.exists(directory):
            #     os.makedirs(directory)
            # path = directory + str(response.url).split("/")[-1]
            #
            # with open(path, "wb") as f:
            #     f.write(pdf)
            yield info


        else: # find  info items

            info['Title'] = self.extract_item(response,self.table_xpath['Title'])
            info['CVE'] = self.extract_item(response, self.table_xpath["CVE"])
            info['EDB_ID'] = self.extract_item(response, self.table_xpath["EDB_ID"])
            info['Author'] = self.extract_item(response, self.table_xpath["AuthorName"])
            info['Published'] = self.extract_item(response, self.table_xpath["Published"])
            info['Type'] = self.extract_item(response, self.table_xpath["Type"])
            info['Platform'] = self.extract_item(response, self.table_xpath["Platform"])
            info['Aliases'] = self.extract_item(response, self.table_xpath["Aliases"])
            info['AdvisorySource'] = self.extract_item(response, self.table_xpath["AdvisorySource"])
            info['Tags'] = self.extract_item(response, self.table_xpath["Tags"])
            info['E_DBVerified'] = self.extract_item(response, self.table_xpath["E_DBVerified"])
            info['VulnerableApp'] = self.extract_item(response, self.table_xpath["VulnerableApp"])
            Exploit_list = (response.xpath(self.table_xpath["Exploit"]).extract())
            Exploit_utf = [x.encode("utf-8") for x in Exploit_list]
            Exploit_base64 = base64.b64encode( re.sub("\xef\xbb\xbf" ,"",re.sub("^\s+","",str("".join(Exploit_utf))).strip()))
            ScreenShot_link = self.extract_item(response, self.table_xpath["ScreenShot"])
            info["Exploit"] = Exploit_base64
            info['ExploitHash'] = hashlib.md5(Exploit_base64).hexdigest()
            info['callback'] = self.callback
            if ScreenShot_link:
                ScreenShot_req = scrapy.Request("https://www.exploit-db.com" + ScreenShot_link, self.save_screenshot)
                ScreenShot_req.meta['item'] = info
                yield ScreenShot_req
            else:
                item = ExploitDbItem()
                for key in info.keys():
                    if "N/A" not in info[key] and len(info[key]):
                        item[key] = re.sub("^:\s?", "", info[key])
                yield item

    def save_screenshot(self,response):
        info = response.meta['item']
        com = base64.b64encode(response.body)
        hashsum = hashlib.md5(com).hexdigest()

        # directory = os.getcwd() + "/ExploitDB/Screen/"
        # if not os.path.exists(directory):
        #     os.makedirs(directory)
        # path = directory + str(response.url).split("/")[-1]
        # Screen = (response.body)
        # with open(path, "wb") as f:
        #     f.write(Screen)
        item = ExploitDbItem()
        for key in info.keys():
            if "N/A" not in info[key] and len(info[key]):
                item[key] = re.sub("^:\s?", "", info[key])
        #item['ScreenShot'] = com
        item['ScreenShotHash'] = hashsum
        yield item








